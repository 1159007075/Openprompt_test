{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path=\"/home/luyy/projects/PTR/data/tacred/train.txt\"\n",
    "import json\n",
    "file=open(train_path,'r')\n",
    "lines=file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-18 17:47:02.916918: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-18 17:47:03.139775: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-18 17:47:03.139803: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-18 17:47:03.999062: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 17:47:03.999168: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-18 17:47:03.999181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaModel\n",
    "model_classes = {'roberta': {'config': RobertaConfig,'tokenizer': RobertaTokenizer,'model':RobertaModel,}}\n",
    "model_config = model_classes['roberta']\n",
    "tokenizer = model_config['tokenizer'].from_pretrained('roberta-large')\n",
    "special=[]\n",
    "tokenizer.add_tokens(special)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1301, 1073,  415])\n",
      "{'token': ['Zagat', 'Survey', ',', 'the', 'guide', 'empire', 'that', 'started', 'as', 'a', 'hobby', 'for', 'Tim', 'and', 'Nina', 'Zagat', 'in', '1979', 'as', 'a', 'two-page', 'typed', 'list', 'of', 'New', 'York', 'restaurants', 'compiled', 'from', 'reviews', 'from', 'friends', ',', 'has', 'been', 'put', 'up', 'for', 'sale', ',', 'according', 'to', 'people', 'briefed', 'on', 'the', 'decision', '.'], 'h': {'name': 'Zagat', 'pos': [0, 1]}, 't': {'name': '1979', 'pos': [17, 18]}, 'relation': 'org:founded'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "for line in lines:\n",
    "  dic=json.loads(line)\n",
    "  input_token = tokenizer.tokenize(dic['h']['name'])\n",
    "  word_ids=tokenizer.convert_tokens_to_ids(input_token)\n",
    "  print(torch.tensor(word_ids))\n",
    "  print(dic)\n",
    "  break\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = model_config['model'].from_pretrained(\"roberta-large\",return_dict=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134, 1024])\n"
     ]
    }
   ],
   "source": [
    "class_vec=0\n",
    "for line in lines:\n",
    "  dic=json.loads(line)\n",
    "  if dic['relation']=='per:date_of_death':\n",
    "    input_token = tokenizer.tokenize(dic['t']['name'])\n",
    "    word_ids=tokenizer.convert_tokens_to_ids(input_token)\n",
    "    embeddings = model.embeddings.word_embeddings(torch.tensor(word_ids))\n",
    "    word_vec=torch.mean(embeddings,dim=0).unsqueeze(0)\n",
    "    if isinstance(class_vec,int):\n",
    "      class_vec=word_vec\n",
    "    else:\n",
    "      class_vec=torch.cat((class_vec,word_vec),0)\n",
    "print(class_vec.size())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RobertaModel' object has no attribute 'most_similar'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m class_vec\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mmean(class_vec,dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmost_similar\u001B[49m(positive\u001B[38;5;241m=\u001B[39m[class_vec], topn\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1185\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1183\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1185\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   1186\u001B[0m     \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, name))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'RobertaModel' object has no attribute 'most_similar'"
     ]
    }
   ],
   "source": [
    "class_vec1=0\n",
    "for line in lines:\n",
    "  dic=json.loads(line)\n",
    "  if dic['relation']=='per:date_of_death':\n",
    "    input_token = tokenizer.tokenize(dic['t']['name'])\n",
    "    word_ids=tokenizer.convert_tokens_to_ids(input_token)\n",
    "    embeddings = model.embeddings.word_embeddings(torch.tensor(word_ids))\n",
    "    word_vec=torch.mean(embeddings,dim=0).unsqueeze(0)\n",
    "    if isinstance(class_vec1,int):\n",
    "      class_vec1=word_vec\n",
    "    else:\n",
    "      class_vec1=torch.cat((class_vec,word_vec),0)\n",
    "print(class_vec1.size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
